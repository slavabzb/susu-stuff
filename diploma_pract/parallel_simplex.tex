\subsection{Распараллеливание симплекс-метода}
\label{parallel_simplex}

Существующие подходы к распараллеливанию симплекс-метода и ему подобных удобно классифицировать по виду симплекс-метода и по использованию разреженных типов данных. Такая классификация позитивно коррелирует с практической ценностью реализации в контексте решения ЗЛП и негативно с успешностью этих подходов в достигнутом ускорении.

Некоторые из рассматриваемых ниже схем предлагают неплохое ускорение относительно эффективных последовательных решателей своего времени. Другие только кажутся неэффективными в свете последовательного обратного симплекса, который к тому моменту либо был малоизвестен, либо был разработан впоследствии. Такие случаи определяются ниже, чтобы подчеркнуть, что в результате огромного увеличения эффективности последовательного обратного симплекс-метода (как во время исследований в области распараллеливания симплекса, так и после) проблема разработки практического параллельного симплекс-решателя стала очень актуальной.

\subsubsection{Параллельный симплекс без использования разреженных матриц}
\label{parallel_simplex_dense}

Стандартный и обратный симплекс-методы без использования алгебры разреженных матриц реализовывались неоднократно. Простота обычных структур данных и потенциал достичь линейного ускорения делают их привлекательными для применения в параллельных вычислениях. Хотя при решении общих разреженных ЗЛП больших размерностей такие реализации малоэффективны, поскольку они могут соперничать с эффективными последовательными реализациями обратного симплекс-метода, использующими разреженные структуры, только при подключении значительного числа процессоров.

Первые работы в этом направлении ограничиваются обсуждением схем распределения данных и коммуникации; реализации ограничиваются небольшим числом процессов на ЭВМ с распределенной памятью (краткие обзоры даются в \cite{Zenios1989, Luo1991}, примеры других ранних работ можно найти в \cite{Finkel1987, Boffley1989, Babaev1991, Agrawal1989}). В одной из относительно ранних работ \cite{Stunkel1988}, в которой были реализованы обычный и обратный симплекс-методы на 16-процессорном Intel hypercube, достигнутое ускорение варьируется от 8 до 12 для небольших задач из библиотеки Netlib. В \cite{Cvetanovic1991} сообщается о 12-кратном ускорении при решении двух небольших ЗЛП с использованием обычного симлекс-метода на 16-процессорной ЭВМ с \textit{общей разделяемой памятью}. Также были случаи получения более чем 12-кратного ускорения \cite{Luo1992}.

В \cite{Eckstein1995} разработаны параллельный обычный и обратный симплекс-методы с применением метода наиболее крутого ребра \cite{Goldfarb1977} и протестированы на машинах Connection Machine CM-2 и CM-5 с массовым параллелизмом. Решая некоторые ЗЛП средней размерности из Netlib и очень плотные задачи машинного обучения, ускорение между 1.6 и 1.8 быо достигнуто только при удвоении число процессоров. В \cite{Thomadakis1996} также используется метод наиболее крутого ребра и обычный симплекс-метод на ЭВМ MasPar MP-1 и MP-2. Решая в основном случайно сгенерированные ЗЛП большой размерности, авторы достигали практически троекратного ускорения. Одной из более поздних работ по реализации параллельного обычного симплекс-метода с запуском на небольшом количестве процессоров является \cite{Yarmish2001}.

Работы по созданию небольших параллельных реализаций обычного симплекс-метода без применения разреженных структур данных продолжаются. Были представлены результаты реализации на 8 процессорах с 5-кратным ускорением при решении небольших случайных ЗЛП \cite{Badr2006}.

\subsubsection{Параллельный симплекс с использованием разреженных матриц}
\label{parallel_simplex_sparse}

Особую сложность в разработке действительно хорошего в практическом смысле параллельного симплекс-метода является применение эффективных техник работы с разреженными матрицами. Разработанный параллельный решатель будет конкурентноспособным по отношению к хорошей последовательной реализации только тогда, когда решение общих разреженных ЗЛП большой размерности будет затрагивать разумное число процессоров.

В период, когда распараллеливание симплекс-метода только начиналось и широко дискутировалось, практические параллельные методы факторизации и решения разреженных асимметричных СЛАУ были только на стадии становления. Как следствие, несмотря на то, что в симплекс-методе с плотными матрицами внедрение параллелизма проходило успешно, было распространено мнение, что использование разреженных матриц сильно ограничивает возможности распараллеливания (за исключением \texttt{PRICE}). Некоторых это наводило на мысль, что разработать хорошую параллельную реализацию невозможно в принципе. Но несмотря на преимущественно последовательную природу компонентов обратного симплекс метода, все еще существуют возможности применения параллелизма по задачам.

\subsubsection{Схема распараллеливания}
\label{parallel_simplex_scheme}

Рассмотрим следующий подход, использующий некоторые (но не все) возможности применения параллелизма по данным и по задачам к алгоритму обратного симплекс-метода с субоптимизацией\cite{Hall2012}. Рисунок~\ref{i:revised_parallel_schema} иллюстрирует идею.

\begin{figure}
\centering
\includegraphics[scale=0.3]{i/revised_parallel_schema.png} 
\caption{Прототип параллельной реализации обратного симплекс-метода с субоптимизацией\label{i:revised_parallel_schema}}
\end{figure}

Сначала относительно дешевая операция CHUZR выбора множества~$\mathcal{P}$ хороших кандидатов для вывода из базиса выполняется на одном ядре. Затем, несколько операций BTRAN~($\pi^T_p=e^T_pB^{-1}$) и PRICE~($\hat{a}^T_p=\pi^T_pN$) для~$p\in\mathcal{P}$ распределяются между всеми ядрами. Поскольку малый цикл итераций обрабатывает только небольшую часть строк, операция CHUZR\_MI выполняется на одном ядре и не показана на рисунке~\ref{i:revised_parallel_schema}. Выбор вводимой колонки выполняется в CHUZC относительно просто, поэтому также не распределяется. Малый цикл замыкает операция UPDATE\_MI, в которой параллельное обновление данных в строках таблицы обратного симплекс-метода (оставшиеся кандидаты) и альтернативных издержек~$\hat{c}^T_N$ выполняется на всех доступных ядрах. Простое обновление~$\hat{b}_P$ выполняется на одном ядре операцией CHUZR\_MI. После завершения малого цикла итераций, операции FTRAN~$\hat{a}=B^{-1}a_q$ и~$\tau=B^{-1}\hat{a}_q$ для каждого изменения базиса распределяется между всеми ядрами. Если необходимо, INVERT выполняется последовательно, без перекрытия любых других вычислений.
