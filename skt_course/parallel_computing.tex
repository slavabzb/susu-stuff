\subsection{Параллельные вычисления}
\label{parallel_computing}

Прежде чем переходить к вопросу распараллеливания симплекс-метода, необходимо рассмотреть некоторые термины и концепции из области параллельного программирования. В этом разделе представлен краткий обзор необходимых понятий. Полное и более общее введение в параллельные вычисления можно найти в\cite{Kumar2003}.

Классифицируя архитектуры параллельных мультипроцессоров, необходимо понимать важное отличие между \textit{распределенной памятью}, когда каждый процессор имеет свою собственную локальную память, и \textit{общей памятью}, когда все процессоры имеют доступ к общей разделяемой памяти. Современные мощные ЭВМ могут состоять из множества распределенных кластеров, каждый из которых может иметь множество процессоров с общей памятью. На более простых мультипроцессорах память может быть либо общей, либо разделяемой.

Обычно успешность распараллеливания измеряется в терминах \textit{ускорения} -- отношения времени, необходимого для решения задачи с использованием более одного процессора, ко времени решения задачи на одном процессоре. Традиционной является цель достичь фактор ускорения, равный количеству подключаемых процессоров. Такой фактор называется \textit{линейным ускорением} и соответствует 100\% \textit{параллельной эффективности}. Увеличение доступной кэш-памяти и оперативной памяти одновременно с количеством процессоров иногда приводит к феномену \textit{сверхлинейного ускорения}. Схемы распараллеливания, для которых (по крайней мере в теории) производительность растет линейно без ограничений с ростом количества подключаемых процессоров, называются \textit{масштабируемыми} схемами. Если параллелизм не используется во всех главных операциях алгоритма, то ускорение, в соответствии с законом Амдала\cite{Amdahl1967}, ограничено долей времени выполнения непараллельных операций.

Существуют две основных парадигмы параллельного программирования. Если работа большинства операций алгоритма может быть распределена среди множества процессоров, тогда говорят о \textit{параллелизме по данным}. В противоположность этому, если возможно выполнять несколько главных операций алгоритма одновременно, тогда имеет место \textit{параллелизм по задачам}. На практике возможно применять одновременно оба подхода для определенного набора главных операций алгоритма.

Есть два фундаментальных способа реализации алгоритмов на параллельных ЭВМ. На машинах с распределенной памятью передача данных между процессорами осуществляется посредством инструкций, порожденных явными вызовами методов \textit{передачи сообщений}. На машинах с общей разделяемой памятью применяется \textit{параллелизм по данным}, когда инструкции записываются как для последовательного исполнения, но транслируются специальным компилятором в параллельный код. Большинство протоколов передачи сообщений также поддерживаются на ЭВМ с общей памятью, равно как и распараллеливание по данным возможно на ЭВМ с распределенной памятью.

На машинах с распределенной памятью, накладные расходы на передачу сообщений между процессорами определяются \textit{задержкой} и \textit{пропускной способностью канала}. Первое -- это время передачи, не зависящее от размера сообщения, а второе -- это скорость связи. Для общих протоколов передачи сообщений задержка и пропускная способность на определенной архитектуре может быть значительно выше, чем в независящей от архитектуры среде, что обычно регулируется и настраивается поставщиком. Если алгоритму для вычислений необходим интенсивный обмен информацией, растущие накладные расходы на связь могут перевесить любые улучшения от использования дополнительных процессоров.
